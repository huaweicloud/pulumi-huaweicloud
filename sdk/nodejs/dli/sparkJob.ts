// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs } from "../types";
import * as utilities from "../utilities";

/**
 * Manages spark job resource of DLI within HuaweiCloud
 *
 * ## Example Usage
 * ### Submit a new spark job with jar packages
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as pulumi from "@huaweicloudos/pulumi";
 *
 * const _default = new huaweicloud.dli.SparkJob("default", {
 *     queueName: _var.queue_name,
 *     appName: "driver_package/driver_behavior.jar",
 *     mainClass: "driver_behavior",
 *     specification: "B",
 *     maxRetries: 20,
 * });
 * ```
 */
export class SparkJob extends pulumi.CustomResource {
    /**
     * Get an existing SparkJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: SparkJobState, opts?: pulumi.CustomResourceOptions): SparkJob {
        return new SparkJob(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'huaweicloud:Dli/sparkJob:SparkJob';

    /**
     * Returns true if the given object is an instance of SparkJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is SparkJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === SparkJob.__pulumiType;
    }

    /**
     * Specifies the name of the package that is of the JAR or python file type and
     * has been uploaded to the DLI resource management system.
     * The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    public readonly appName!: pulumi.Output<string>;
    /**
     * Specifies the input parameters of the main class.
     * Changing this parameter will submit a new spark job.
     */
    public readonly appParameters!: pulumi.Output<string | undefined>;
    /**
     * Specifies the configuration items of the DLI spark.
     * Please following the document of Spark [configurations](https://spark.apache.org/docs/latest/configuration.html) for
     * this argument. If you want to enable the `access metadata` of DLI spark in HuaweiCloud, please set
     * `spark.dli.metaAccess.enable` to `true`. Changing this parameter will submit a new spark job.
     */
    public readonly configurations!: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * Time of the DLI spark job submit.
     */
    public /*out*/ readonly createdAt!: pulumi.Output<string>;
    /**
     * Specifies a list of package resource objects.
     * The object structure is documented below.
     * Changing this parameter will submit a new spark job.
     */
    public readonly dependentPackages!: pulumi.Output<outputs.Dli.SparkJobDependentPackage[] | undefined>;
    /**
     * Specifies the number of CPU cores of the Spark application driver.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly driverCores!: pulumi.Output<number | undefined>;
    /**
     * Specifies the driver memory of the spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly driverMemory!: pulumi.Output<string | undefined>;
    /**
     * Specifies the number of CPU cores of each executor in the Spark
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly executorCores!: pulumi.Output<number | undefined>;
    /**
     * Specifies the executor memory of the spark application.
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly executorMemory!: pulumi.Output<string | undefined>;
    /**
     * Specifies the number of executors in a spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly executors!: pulumi.Output<number | undefined>;
    /**
     * Specifies a list of the other dependencies name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<dependent files>`.
     * Changing this parameter will submit a new spark job.
     */
    public readonly files!: pulumi.Output<string[] | undefined>;
    /**
     * Specifies a list of the jar package name which has been uploaded to the DLI
     * resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    public readonly jars!: pulumi.Output<string[] | undefined>;
    /**
     * Specifies the main class of the spark job.
     * Required if the `appName` is the JAR type.
     * Changing this parameter will submit a new spark job.
     */
    public readonly mainClass!: pulumi.Output<string | undefined>;
    /**
     * Specifies the maximum retry times.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    public readonly maxRetries!: pulumi.Output<number | undefined>;
    /**
     * Specifies a list of modules that depend on system resources.
     * The dependent modules and corresponding services are as follows.
     * Changing this parameter will submit a new spark job.
     * + **sys.datasource.hbase**: CloudTable/MRS HBase
     * + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
     * + **sys.datasource.rds**: RDS MySQL
     * + **sys.datasource.css**: CSS
     */
    public readonly modules!: pulumi.Output<string[] | undefined>;
    /**
     * Specifies the spark job name.
     * The value contains a maximum of 128 characters.
     * Changing this parameter will submit a new spark job.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * The owner of the spark job.
     */
    public /*out*/ readonly owner!: pulumi.Output<string>;
    /**
     * Specifies a list of the python file name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<python file name>`.
     * Changing this parameter will submit a new spark job.
     */
    public readonly pythonFiles!: pulumi.Output<string[] | undefined>;
    /**
     * Specifies the DLI queue name.
     * Changing this parameter will submit a new spark job.
     */
    public readonly queueName!: pulumi.Output<string>;
    /**
     * Specifies the region in which to submit a spark job.
     * If omitted, the provider-level region will be used.
     * Changing this parameter will submit a new spark job.
     */
    public readonly region!: pulumi.Output<string>;
    /**
     * Specifies the compute resource type for spark application.
     * The available types and related specifications are as follows, default to minimum configuration (type **A**).
     * Changing this parameter will submit a new spark job.
     */
    public readonly specification!: pulumi.Output<string | undefined>;

    /**
     * Create a SparkJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: SparkJobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: SparkJobArgs | SparkJobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as SparkJobState | undefined;
            resourceInputs["appName"] = state ? state.appName : undefined;
            resourceInputs["appParameters"] = state ? state.appParameters : undefined;
            resourceInputs["configurations"] = state ? state.configurations : undefined;
            resourceInputs["createdAt"] = state ? state.createdAt : undefined;
            resourceInputs["dependentPackages"] = state ? state.dependentPackages : undefined;
            resourceInputs["driverCores"] = state ? state.driverCores : undefined;
            resourceInputs["driverMemory"] = state ? state.driverMemory : undefined;
            resourceInputs["executorCores"] = state ? state.executorCores : undefined;
            resourceInputs["executorMemory"] = state ? state.executorMemory : undefined;
            resourceInputs["executors"] = state ? state.executors : undefined;
            resourceInputs["files"] = state ? state.files : undefined;
            resourceInputs["jars"] = state ? state.jars : undefined;
            resourceInputs["mainClass"] = state ? state.mainClass : undefined;
            resourceInputs["maxRetries"] = state ? state.maxRetries : undefined;
            resourceInputs["modules"] = state ? state.modules : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["owner"] = state ? state.owner : undefined;
            resourceInputs["pythonFiles"] = state ? state.pythonFiles : undefined;
            resourceInputs["queueName"] = state ? state.queueName : undefined;
            resourceInputs["region"] = state ? state.region : undefined;
            resourceInputs["specification"] = state ? state.specification : undefined;
        } else {
            const args = argsOrState as SparkJobArgs | undefined;
            if ((!args || args.appName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'appName'");
            }
            if ((!args || args.queueName === undefined) && !opts.urn) {
                throw new Error("Missing required property 'queueName'");
            }
            resourceInputs["appName"] = args ? args.appName : undefined;
            resourceInputs["appParameters"] = args ? args.appParameters : undefined;
            resourceInputs["configurations"] = args ? args.configurations : undefined;
            resourceInputs["dependentPackages"] = args ? args.dependentPackages : undefined;
            resourceInputs["driverCores"] = args ? args.driverCores : undefined;
            resourceInputs["driverMemory"] = args ? args.driverMemory : undefined;
            resourceInputs["executorCores"] = args ? args.executorCores : undefined;
            resourceInputs["executorMemory"] = args ? args.executorMemory : undefined;
            resourceInputs["executors"] = args ? args.executors : undefined;
            resourceInputs["files"] = args ? args.files : undefined;
            resourceInputs["jars"] = args ? args.jars : undefined;
            resourceInputs["mainClass"] = args ? args.mainClass : undefined;
            resourceInputs["maxRetries"] = args ? args.maxRetries : undefined;
            resourceInputs["modules"] = args ? args.modules : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["pythonFiles"] = args ? args.pythonFiles : undefined;
            resourceInputs["queueName"] = args ? args.queueName : undefined;
            resourceInputs["region"] = args ? args.region : undefined;
            resourceInputs["specification"] = args ? args.specification : undefined;
            resourceInputs["createdAt"] = undefined /*out*/;
            resourceInputs["owner"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(SparkJob.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering SparkJob resources.
 */
export interface SparkJobState {
    /**
     * Specifies the name of the package that is of the JAR or python file type and
     * has been uploaded to the DLI resource management system.
     * The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    appName?: pulumi.Input<string>;
    /**
     * Specifies the input parameters of the main class.
     * Changing this parameter will submit a new spark job.
     */
    appParameters?: pulumi.Input<string>;
    /**
     * Specifies the configuration items of the DLI spark.
     * Please following the document of Spark [configurations](https://spark.apache.org/docs/latest/configuration.html) for
     * this argument. If you want to enable the `access metadata` of DLI spark in HuaweiCloud, please set
     * `spark.dli.metaAccess.enable` to `true`. Changing this parameter will submit a new spark job.
     */
    configurations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Time of the DLI spark job submit.
     */
    createdAt?: pulumi.Input<string>;
    /**
     * Specifies a list of package resource objects.
     * The object structure is documented below.
     * Changing this parameter will submit a new spark job.
     */
    dependentPackages?: pulumi.Input<pulumi.Input<inputs.Dli.SparkJobDependentPackage>[]>;
    /**
     * Specifies the number of CPU cores of the Spark application driver.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    driverCores?: pulumi.Input<number>;
    /**
     * Specifies the driver memory of the spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    driverMemory?: pulumi.Input<string>;
    /**
     * Specifies the number of CPU cores of each executor in the Spark
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executorCores?: pulumi.Input<number>;
    /**
     * Specifies the executor memory of the spark application.
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executorMemory?: pulumi.Input<string>;
    /**
     * Specifies the number of executors in a spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executors?: pulumi.Input<number>;
    /**
     * Specifies a list of the other dependencies name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<dependent files>`.
     * Changing this parameter will submit a new spark job.
     */
    files?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies a list of the jar package name which has been uploaded to the DLI
     * resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    jars?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the main class of the spark job.
     * Required if the `appName` is the JAR type.
     * Changing this parameter will submit a new spark job.
     */
    mainClass?: pulumi.Input<string>;
    /**
     * Specifies the maximum retry times.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    maxRetries?: pulumi.Input<number>;
    /**
     * Specifies a list of modules that depend on system resources.
     * The dependent modules and corresponding services are as follows.
     * Changing this parameter will submit a new spark job.
     * + **sys.datasource.hbase**: CloudTable/MRS HBase
     * + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
     * + **sys.datasource.rds**: RDS MySQL
     * + **sys.datasource.css**: CSS
     */
    modules?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the spark job name.
     * The value contains a maximum of 128 characters.
     * Changing this parameter will submit a new spark job.
     */
    name?: pulumi.Input<string>;
    /**
     * The owner of the spark job.
     */
    owner?: pulumi.Input<string>;
    /**
     * Specifies a list of the python file name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<python file name>`.
     * Changing this parameter will submit a new spark job.
     */
    pythonFiles?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the DLI queue name.
     * Changing this parameter will submit a new spark job.
     */
    queueName?: pulumi.Input<string>;
    /**
     * Specifies the region in which to submit a spark job.
     * If omitted, the provider-level region will be used.
     * Changing this parameter will submit a new spark job.
     */
    region?: pulumi.Input<string>;
    /**
     * Specifies the compute resource type for spark application.
     * The available types and related specifications are as follows, default to minimum configuration (type **A**).
     * Changing this parameter will submit a new spark job.
     */
    specification?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a SparkJob resource.
 */
export interface SparkJobArgs {
    /**
     * Specifies the name of the package that is of the JAR or python file type and
     * has been uploaded to the DLI resource management system.
     * The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    appName: pulumi.Input<string>;
    /**
     * Specifies the input parameters of the main class.
     * Changing this parameter will submit a new spark job.
     */
    appParameters?: pulumi.Input<string>;
    /**
     * Specifies the configuration items of the DLI spark.
     * Please following the document of Spark [configurations](https://spark.apache.org/docs/latest/configuration.html) for
     * this argument. If you want to enable the `access metadata` of DLI spark in HuaweiCloud, please set
     * `spark.dli.metaAccess.enable` to `true`. Changing this parameter will submit a new spark job.
     */
    configurations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Specifies a list of package resource objects.
     * The object structure is documented below.
     * Changing this parameter will submit a new spark job.
     */
    dependentPackages?: pulumi.Input<pulumi.Input<inputs.Dli.SparkJobDependentPackage>[]>;
    /**
     * Specifies the number of CPU cores of the Spark application driver.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    driverCores?: pulumi.Input<number>;
    /**
     * Specifies the driver memory of the spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    driverMemory?: pulumi.Input<string>;
    /**
     * Specifies the number of CPU cores of each executor in the Spark
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executorCores?: pulumi.Input<number>;
    /**
     * Specifies the executor memory of the spark application.
     * application. The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executorMemory?: pulumi.Input<string>;
    /**
     * Specifies the number of executors in a spark application.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    executors?: pulumi.Input<number>;
    /**
     * Specifies a list of the other dependencies name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<dependent files>`.
     * Changing this parameter will submit a new spark job.
     */
    files?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies a list of the jar package name which has been uploaded to the DLI
     * resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<package name>`.
     * Changing this parameter will submit a new spark job.
     */
    jars?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the main class of the spark job.
     * Required if the `appName` is the JAR type.
     * Changing this parameter will submit a new spark job.
     */
    mainClass?: pulumi.Input<string>;
    /**
     * Specifies the maximum retry times.
     * The default value of this value corresponds to the configuration of the selected `specification`.
     * If you set this value instead of the default value, `specification` will be invalid.
     * Changing this parameter will submit a new spark job.
     */
    maxRetries?: pulumi.Input<number>;
    /**
     * Specifies a list of modules that depend on system resources.
     * The dependent modules and corresponding services are as follows.
     * Changing this parameter will submit a new spark job.
     * + **sys.datasource.hbase**: CloudTable/MRS HBase
     * + **sys.datasource.opentsdb**: CloudTable/MRS OpenTSDB
     * + **sys.datasource.rds**: RDS MySQL
     * + **sys.datasource.css**: CSS
     */
    modules?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the spark job name.
     * The value contains a maximum of 128 characters.
     * Changing this parameter will submit a new spark job.
     */
    name?: pulumi.Input<string>;
    /**
     * Specifies a list of the python file name which has been uploaded to the
     * DLI resource management system. The OBS paths are allowed, for example, `obs://<bucket name>/<python file name>`.
     * Changing this parameter will submit a new spark job.
     */
    pythonFiles?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies the DLI queue name.
     * Changing this parameter will submit a new spark job.
     */
    queueName: pulumi.Input<string>;
    /**
     * Specifies the region in which to submit a spark job.
     * If omitted, the provider-level region will be used.
     * Changing this parameter will submit a new spark job.
     */
    region?: pulumi.Input<string>;
    /**
     * Specifies the compute resource type for spark application.
     * The available types and related specifications are as follows, default to minimum configuration (type **A**).
     * Changing this parameter will submit a new spark job.
     */
    specification?: pulumi.Input<string>;
}
